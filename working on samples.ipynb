{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_key_map():\n",
    "    a = np.arange(0,10)\n",
    "    b = a\n",
    "    dic = {}\n",
    "    for i in a:\n",
    "        for j in b:\n",
    "            dic[i + j] = 1\n",
    "            dic[i * j] = 1\n",
    "\n",
    "    key_map = {}\n",
    "    index = 0\n",
    "    for key in dic:\n",
    "        key_map[key] = index\n",
    "        index += 1\n",
    "    return key_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data is a list of int\n",
    "def OneHotKey(data, key_map):\n",
    "    key_size = len(key_map)\n",
    "    data_size = len(data)\n",
    "    result = np.zeros([data_size, key_size])\n",
    "    for i in xrange(0, data_size):\n",
    "        result[i][key_map[int(data[i])]] = 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1. define the structure\n",
    "#2. given training datas and training labels, train the neural_network.\n",
    "#3. given test datas, predict the output\n",
    "class neural_network:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.structure = [] #store the structure of the nn\n",
    "        self.weights = [] #store the weights of each layer. Note: weights of each layer is a matrix.\n",
    "        self.biases = [] # store the bias of each layer\n",
    "        self.outputs = [] # store each layer's output, the first layer is the input.\n",
    "        self.gradients_w = [] # store the gradient of weights of each layer. it should has the same dimension as self.weights\n",
    "        self.gradients_b = [] # store the gradient of bias of each layer. it should has the smae dimension as self.bias\n",
    "        self.alpha = 0.1 # the learning rate\n",
    "\n",
    "    #using a list structure to define the structure of the network\n",
    "    #e.g [2,3,2] means the input layer has two nodes, the hidden layer has three nodes and the output layer has two nodes.\n",
    "    def def_structure(self, structure):\n",
    "        self.structure = structure\n",
    "        self.outputs.append(np.zeros([structure[0],1]))\n",
    "        for i in xrange(0 , len(structure) - 1):\n",
    "            self.weights.append(np.random.randn(structure[i + 1], structure[i]))\n",
    "            self.gradients_w.append(np.zeros([structure[i + 1], structure[i]]))\n",
    "            self.biases.append(np.random.randn(structure[i + 1],1))\n",
    "            self.gradients_b.append(np.zeros([structure[i + 1],1]))\n",
    "            self.outputs.append(np.zeros([structure[i + 1],1]))\n",
    "        self.weights = np.array(self.weights)\n",
    "        self.biases = np.array(self.biases)\n",
    "        self.gradients_w = np.array(self.gradients_w)\n",
    "        self.gradients_b = np.array(self.gradients_b)\n",
    "        self.outputs = np.array(self.outputs)\n",
    "    \n",
    "    \n",
    "    # compute the forward result of the nn and record output of each layer\n",
    "    def forward(self, data):\n",
    "        data = data.reshape(-1, 1)\n",
    "        sum_product = data\n",
    "        self.outputs[0] = data\n",
    "        index = 1\n",
    "        for (w , b) in zip(self.weights, self.biases):\n",
    "            sum_product = np.dot(w, sum_product) + b\n",
    "            sum_product = sigmoid(sum_product)\n",
    "            self.outputs[index] = sum_product\n",
    "            index += 1\n",
    "        return sum_product\n",
    "    \n",
    "    # compute gradient of weights and biases of each layer\n",
    "    def backpropogate(self, labels):\n",
    "        #output layer error:\n",
    "        labels = labels.reshape(-1,1)\n",
    "        sum_grad = self.outputs[-1] - labels \n",
    "        grad = np.dot(sum_grad, self.outputs[-2].T)\n",
    "        self.gradients_w[-1] += grad\n",
    "        self.gradients_b[-1] += sum_grad\n",
    "\n",
    "        #layer error of the rest layers.\n",
    "        layer_num = len(self.structure)\n",
    "        for i in xrange(layer_num - 2, 0, -1):\n",
    "            node_grad = np.dot(sum_grad.T, self.weights[i]).T\n",
    "            sum_grad = node_grad * self.outputs[i] * (1 - self.outputs[i])\n",
    "            grad = np.dot(sum_grad, self.outputs[i - 1].T)\n",
    "            self.gradients_w[i - 1] += grad\n",
    "            self.gradients_b[i - 1] += sum_grad\n",
    "\n",
    "    #train network on mini_batch\n",
    "    #first, compute the forward result and record\n",
    "    #second, compute the gradient of each weights and biases\n",
    "    #thrid, updata the network\n",
    "    def train_mini_batch(self, batch_datas, batch_labels):\n",
    "        self.gradients_w.fill(0)\n",
    "        self.gradients_b.fill(0)\n",
    "        for data, label in zip(batch_datas, batch_labels):\n",
    "            self.forward(data)\n",
    "            self.backpropogate(label)\n",
    "        self.weights -= self.alpha * self.gradients_w / len(batch_datas)\n",
    "        self.biases -= self.alpha * self.gradients_b / len(batch_datas)\n",
    "        #print self.weights\n",
    "\n",
    "    #train_data: numpy_array, n*m, n is the number of items, m is the number of features.\n",
    "    #train_labels: numpy_array, n*1, n is the number of items.\n",
    "    #first, shuffle and split the train_data and train_label\n",
    "    #second, train network on each mini_batch\n",
    "    def fit(self, train_datas, train_labels, epochs = 10, batch_size = 64, learning_rate = 0.1):\n",
    "        self.alpha = learning_rate\n",
    "        data_size = len(train_datas)\n",
    "        for epoch in xrange(0, epochs):\n",
    "            index = np.arange(0, data_size)\n",
    "            np.random.shuffle(index)\n",
    "            start_data = 0\n",
    "            print(\"epoch : {}\\n\".format(epoch))\n",
    "            while (start_data < data_size):\n",
    "                end_data = min(start_data + batch_size, data_size)\n",
    "                batch_datas = train_datas[index[start_data: end_data]]\n",
    "                batch_labels = train_labels[index[start_data: end_data]]\n",
    "                self.train_mini_batch(batch_datas, batch_labels)\n",
    "                start_data += batch_size\n",
    "            print(\"training error : {}\".format(self.evaluate(train_datas, train_labels)))\n",
    "    \n",
    "    def predict(self, data):\n",
    "        return self.forward(data)\n",
    "    \n",
    "    def evaluate(self, datas, labels):\n",
    "        error = 0\n",
    "        for data, label in zip(datas, labels):\n",
    "            label = label.reshape(-1,1)\n",
    "            error += sum(abs(self.predict(data) - label))\n",
    "            #print error\n",
    "        return error / len(datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_x = pickle.load(open(\"sample_train_x\"))\n",
    "sample_y = pickle.load(open(\"sample_train_y\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_x = np.array(sample_x).reshape(-1, 4096)\n",
    "sample_y = np.array(sample_y).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 4096)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "key_map = get_key_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_y = OneHotKey(sample_y, key_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn = neural_network()\n",
    "nn.def_structure([4096, 1024, 40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error : [ 1.98280334]\n",
      "epoch : 1\n",
      "\n",
      "training error : [ 1.95716651]\n",
      "epoch : 2\n",
      "\n",
      "training error : [ 1.94487433]\n",
      "epoch : 3\n",
      "\n",
      "training error : [ 1.80882133]\n",
      "epoch : 4\n",
      "\n",
      "training error : [ 1.89133213]\n",
      "epoch : 5\n",
      "\n",
      "training error : [ 1.83292845]\n",
      "epoch : 6\n",
      "\n",
      "training error : [ 1.81897761]\n",
      "epoch : 7\n",
      "\n",
      "training error : [ 1.82564209]\n",
      "epoch : 8\n",
      "\n",
      "training error : [ 1.90898327]\n",
      "epoch : 9\n",
      "\n",
      "training error : [ 1.80616153]\n",
      "epoch : 10\n",
      "\n",
      "training error : [ 1.79750555]\n",
      "epoch : 11\n",
      "\n",
      "training error : [ 1.7447687]\n",
      "epoch : 12\n",
      "\n",
      "training error : [ 1.82698941]\n",
      "epoch : 13\n",
      "\n",
      "training error : [ 1.80526511]\n",
      "epoch : 14\n",
      "\n",
      "training error : [ 1.71016166]\n",
      "epoch : 15\n",
      "\n",
      "training error : [ 1.80182959]\n",
      "epoch : 16\n",
      "\n",
      "training error : [ 1.77617176]\n",
      "epoch : 17\n",
      "\n",
      "training error : [ 1.70297323]\n",
      "epoch : 18\n",
      "\n",
      "training error : [ 1.77098051]\n",
      "epoch : 19\n",
      "\n",
      "training error : [ 1.67368498]\n",
      "epoch : 20\n",
      "\n",
      "training error : [ 1.73960669]\n",
      "epoch : 21\n",
      "\n",
      "training error : [ 1.61811733]\n",
      "epoch : 23\n",
      "\n",
      "training error : [ 1.78934953]\n",
      "epoch : 24\n",
      "\n",
      "training error : [ 1.66554619]\n",
      "epoch : 25\n",
      "\n",
      "training error : [ 1.5921929]\n",
      "epoch : 26\n",
      "\n",
      "training error : [ 1.64099198]\n",
      "epoch : 27\n",
      "\n",
      "training error : [ 1.68473125]\n",
      "epoch : 28\n",
      "\n",
      "training error : [ 1.61484439]\n",
      "epoch : 29\n",
      "\n",
      "training error : [ 1.60542786]\n",
      "epoch : 30\n",
      "\n",
      "training error : [ 1.56649903]\n",
      "epoch : 31\n",
      "\n",
      "training error : [ 1.49263339]\n",
      "epoch : 32\n",
      "\n",
      "training error : [ 1.548724]\n",
      "epoch : 33\n",
      "\n",
      "training error : [ 1.578206]\n",
      "epoch : 34\n",
      "\n",
      "training error : [ 1.53566324]\n",
      "epoch : 35\n",
      "\n",
      "training error : [ 1.55937942]\n",
      "epoch : 36\n",
      "\n",
      "training error : [ 1.52599327]\n",
      "epoch : 37\n",
      "\n",
      "training error : [ 1.52242441]\n",
      "epoch : 38\n",
      "\n",
      "training error : [ 1.48454063]\n",
      "epoch : 39\n",
      "\n",
      "training error : [ 1.44630945]\n",
      "epoch : 40\n",
      "\n",
      "training error : [ 1.4391064]\n",
      "epoch : 41\n",
      "\n",
      "training error : [ 1.45723104]\n",
      "epoch : 42\n",
      "\n",
      "training error : [ 1.46709626]\n",
      "epoch : 43\n",
      "\n",
      "training error : [ 1.45138126]\n",
      "epoch : 44\n",
      "\n",
      "training error : [ 1.45414068]\n",
      "epoch : 45\n",
      "\n",
      "training error : [ 1.41992068]\n",
      "epoch : 46\n",
      "\n",
      "training error : [ 1.35841364]\n",
      "epoch : 47\n",
      "\n",
      "training error : [ 1.33953256]\n",
      "epoch : 48\n",
      "\n",
      "training error : [ 1.39741772]\n",
      "epoch : 49\n",
      "\n",
      "training error : [ 1.37694124]\n",
      "epoch : 50\n",
      "\n",
      "training error : [ 1.32090984]\n",
      "epoch : 51\n",
      "\n",
      "training error : [ 1.34153456]\n",
      "epoch : 54\n",
      "\n",
      "training error : [ 1.23662372]\n",
      "epoch : 57\n",
      "\n",
      "training error : [ 1.31453783]\n",
      "epoch : 58\n",
      "\n",
      "training error : [ 1.25057389]\n",
      "epoch : 59\n",
      "\n",
      "training error : [ 1.27496658]\n",
      "epoch : 60\n",
      "\n",
      "training error : [ 1.2740414]\n",
      "epoch : 61\n",
      "\n",
      "training error : [ 1.21003614]\n",
      "epoch : 62\n",
      "\n",
      "training error : [ 1.29109484]\n",
      "epoch : 63\n",
      "\n",
      "training error : [ 1.14242164]\n",
      "epoch : 64\n",
      "\n",
      "training error : [ 1.14303047]\n",
      "epoch : 65\n",
      "\n",
      "training error : [ 1.22953591]\n",
      "epoch : 66\n",
      "\n",
      "training error : [ 1.15207174]\n",
      "epoch : 67\n",
      "\n",
      "training error : [ 1.12864514]\n",
      "epoch : 68\n",
      "\n",
      "training error : [ 1.12128508]\n",
      "epoch : 69\n",
      "\n",
      "training error : [ 1.14181597]\n",
      "epoch : 70\n",
      "\n",
      "training error : [ 1.10751141]\n",
      "epoch : 71\n",
      "\n",
      "training error : [ 1.03427356]\n",
      "epoch : 72\n",
      "\n",
      "training error : [ 1.04919348]\n",
      "epoch : 73\n",
      "\n",
      "training error : [ 1.0292422]\n",
      "epoch : 74\n",
      "\n",
      "training error : [ 1.01214576]\n",
      "epoch : 75\n",
      "\n",
      "training error : [ 0.97387574]\n",
      "epoch : 76\n",
      "\n",
      "training error : [ 0.97475868]\n",
      "epoch : 77\n",
      "\n",
      "training error : [ 1.02820148]\n",
      "epoch : 78\n",
      "\n",
      "training error : [ 0.96907022]\n",
      "epoch : 79\n",
      "\n",
      "training error : [ 1.01117462]\n",
      "epoch : 80\n",
      "\n",
      "training error : [ 1.00163409]\n",
      "epoch : 81\n",
      "\n",
      "training error : [ 0.93253787]\n",
      "epoch : 82\n",
      "\n",
      "training error : [ 0.92059925]\n",
      "epoch : 83\n",
      "\n",
      "training error : [ 0.95938206]\n",
      "epoch : 84\n",
      "\n",
      "training error : [ 0.88761734]\n",
      "epoch : 85\n",
      "\n",
      "training error : [ 0.89895732]\n",
      "epoch : 86\n",
      "\n",
      "training error : [ 0.89685155]\n",
      "epoch : 87\n",
      "\n",
      "training error : [ 0.8561447]\n",
      "epoch : 88\n",
      "\n",
      "training error : [ 0.88722162]\n",
      "epoch : 91\n",
      "\n",
      "training error : [ 0.78844032]\n",
      "epoch : 92\n",
      "\n",
      "training error : [ 0.79223418]\n",
      "epoch : 95\n",
      "\n",
      "training error : [ 0.78380952]\n",
      "epoch : 96\n",
      "\n",
      "training error : [ 0.77363644]\n",
      "epoch : 97\n",
      "\n",
      "training error : [ 0.73832615]\n",
      "epoch : 98\n",
      "\n",
      "training error : [ 0.72313125]\n",
      "epoch : 99\n",
      "\n",
      "training error : [ 0.75137742]\n"
     ]
    }
   ],
   "source": [
    "nn.fit(sample_x, sample_y, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights_file = open(\"weights\" , \"w\")\n",
    "pickle.dump(nn.weights, weights_file)\n",
    "weights_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(nn.predict(sample_x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(sample_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
